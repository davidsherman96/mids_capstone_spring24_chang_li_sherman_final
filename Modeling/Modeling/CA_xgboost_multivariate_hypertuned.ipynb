{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost - Hypertuning for Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oeCE85RZaBN0"
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler, StandardScaler\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import datetime\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4dmgFLz8_ySQ"
   },
   "outputs": [],
   "source": [
    "# Set random seed for TensorFlow\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# Set random seed for Python\n",
    "np.random.seed(123)\n",
    "\n",
    "tf.keras.utils.set_random_seed(1)\n",
    "\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_1Ua7mbsbUOr"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_all_county = pd.read_csv('data/CA_data_lat_log_weekly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime to extract month as a feature\n",
    "data_all_county['date'] = pd.to_datetime(data_all_county['date'])\n",
    "data_all_county['month'] = data_all_county['date'].dt.month\n",
    "data_all_county['month'] = data_all_county['month'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkZqApXYXb-9"
   },
   "source": [
    "# Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fIDCIpwSXXKt"
   },
   "outputs": [],
   "source": [
    "def ts_multi_data_prep(dataset, target, start, end, window, step_out):\n",
    "    '''Slices the dataset into sliding and overlapping windows of desired window size and forecast horizon'''\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    start = start + window\n",
    "    if end is None:\n",
    "        end = len(dataset) - step_out\n",
    "        #end = len(dataset)\n",
    "    for i in range(start, end):\n",
    "        indices = range(i-window, i)\n",
    "        X.append(dataset[indices])\n",
    "\n",
    "        indicey = range(i, i+step_out) #revise the window definition\n",
    "        y.append(target[indicey])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vp6wtluEXfZb"
   },
   "outputs": [],
   "source": [
    "def timeseries_evaluation_metrics_func(y_true, y_pred):\n",
    "    '''Calculates and returns the evaluation metrics, MSE and MAE'''\n",
    "    \n",
    "    print('Evaluation metric results:-')\n",
    "    mse = metrics.mean_squared_error(y_true.flatten(), y_pred.flatten())\n",
    "    mae = metrics.mean_absolute_error(y_true.flatten(), y_pred.flatten())\n",
    "    \n",
    "    # Can calculate RMSE and R2 and print results if desired\n",
    "    #rmse = np.sqrt(mse)\n",
    "    #r2 = metrics.r2_score(y_true.flatten(), y_pred.flatten())\n",
    "    #print(f'MSE is : {mse}')\n",
    "    #print(f'MAE is : {mae}')\n",
    "    #print(f'RMSE is : {rmse}')\n",
    "    #print(f'R2 is : {r2}\\n')\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DZJVpSJ_49m2"
   },
   "outputs": [],
   "source": [
    "def timeseries_evaluation_metrics_binary(y_true, y_pred):\n",
    "    '''Calculates and returns the F1 score for the binary classification task'''\n",
    "    \n",
    "    print('Evaluation metric results:-')\n",
    "    accuracy = accuracy_score(y_true.flatten(), y_pred.flatten())\n",
    "    precision = precision_score(y_true.flatten(), y_pred.flatten(), average='macro')\n",
    "    recall = recall_score(y_true.flatten(), y_pred.flatten(), average='macro')\n",
    "    f1 = f1_score(y_true.flatten(), y_pred.flatten(), average='macro')\n",
    "\n",
    "    #print(f'Accuracy: {accuracy}')\n",
    "    #print(f'Precision: {precision}')\n",
    "    #print(f'Recall: {recall}')\n",
    "    #print(f'F1-score: {f1}\\n')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xxvHTDdUqxyN"
   },
   "outputs": [],
   "source": [
    "def transform_county_data(x_data_array, y_data_array):\n",
    "    # Lists to store x_train_c and y_train_c arrays\n",
    "    x_c_list = []\n",
    "    y_c_list = []\n",
    "    # Divide the arrays into 'unique_fips_count' number of subarrays\n",
    "    x_subarrays = np.array_split(x_data_array, unique_fips_count, axis=0)\n",
    "    y_subarrays = np.array_split(y_data_array, unique_fips_count, axis=0)\n",
    "\n",
    "    # Combine x_subarrays and y_subarrays into tuples\n",
    "    data_tuples = [(x_subarray, y_subarray) for x_subarray, y_subarray in zip(x_subarrays, y_subarrays)]\n",
    "\n",
    "    # Print or use the data tuples as needed\n",
    "    for idx, data_tuple in enumerate(data_tuples):\n",
    "        x_window_c, y_window_c = ts_multi_data_prep(data_tuple[0],data_tuple[1], 0, None, hist_window, step_out)\n",
    "        # Append x_window_c and y_window_c arrays to lists\n",
    "        x_c_list.append(x_window_c)\n",
    "        y_c_list.append(y_window_c)\n",
    "\n",
    "    # Stack arrays in lists to create x_train_c and y_train_c\n",
    "    x_all_county = np.vstack(x_c_list)\n",
    "    y_all_county = np.vstack(y_c_list)\n",
    "\n",
    "    return x_all_county, y_all_county"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BJyFdGZFeeVi"
   },
   "outputs": [],
   "source": [
    "# Create empty lists to hold arrays\n",
    "x_train_c, y_train_c, x_vali_c, y_vali_c, x_test_c, y_test_c = [], [], [], [], [], []\n",
    "\n",
    "# Get the unique FIPS or county codes\n",
    "unique_fips = data_all_county['fips'].unique()\n",
    "unique_fips_count = data_all_county['fips'].nunique()\n",
    "\n",
    "for fips in unique_fips:\n",
    "    # Extract dataframe for the current FIPS value\n",
    "    data_county = data_all_county[data_all_county['fips'] == fips]\n",
    "\n",
    "    # Select features to use in the dataset\n",
    "    X_data = data_county[['lat','lon','PRECTOT', 'PS', 'QV2M', 'T2M', 'T2MDEW', 'T2MWET',\n",
    "       'T2M_MAX', 'T2M_MIN', 'T2M_RANGE', 'TS', 'WS10M', 'WS10M_MAX',\n",
    "       'WS10M_MIN', 'WS10M_RANGE', 'WS50M', 'WS50M_MAX', 'WS50M_MIN',\n",
    "       'WS50M_RANGE', 'score', 'month']]\n",
    "    \n",
    "    # Set the labels\n",
    "    Y_data = data_county[['score']]\n",
    "    \n",
    "    #train_val_test split 70%-10%-20%\n",
    "    n = len(X_data)\n",
    "\n",
    "    x_train_county = X_data[0:int(n*0.7)]\n",
    "    y_train_county = Y_data[0:int(n*0.7)]\n",
    "    x_vali_county = X_data[int(n*0.7):int(n*0.8)]\n",
    "    y_vali_county = Y_data[int(n*0.7):int(n*0.8)]\n",
    "    x_test_county = X_data[int(n*0.8):]\n",
    "    y_test_county = Y_data[int(n*0.8):]\n",
    "\n",
    "    # Condition for the first county in CA to start populating the lists\n",
    "    if fips == 6001:\n",
    "        x_train_c, y_train_c, x_vali_c, y_vali_c, x_test_c, y_test_c = x_train_county, y_train_county, x_vali_county, y_vali_county, x_test_county, y_test_county\n",
    "\n",
    "    # Append the following counties to the lists\n",
    "    else:\n",
    "        x_train_c = np.concatenate((x_train_c, x_train_county), axis=0)\n",
    "        y_train_c = np.concatenate((y_train_c, y_train_county), axis=0)\n",
    "        x_vali_c = np.concatenate((x_vali_c, x_vali_county), axis=0)\n",
    "        y_vali_c = np.concatenate((y_vali_c, y_vali_county), axis=0)\n",
    "        x_test_c = np.concatenate((x_test_c, x_test_county), axis=0)\n",
    "        y_test_c = np.concatenate((y_test_c, y_test_county), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MjYfAZXoYPE7"
   },
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_scaler_train = MinMaxScaler()\n",
    "Y_scaler_train = MinMaxScaler()\n",
    "X_scaler_test = MinMaxScaler()\n",
    "Y_scaler_test = MinMaxScaler()\n",
    "X_scaler_vali = MinMaxScaler()\n",
    "Y_scaler_vali = MinMaxScaler()\n",
    "x_train_data = X_scaler_train.fit_transform(x_train_c)\n",
    "y_train_data = Y_scaler_train.fit_transform(y_train_c)\n",
    "x_vali_data = X_scaler_vali.fit_transform(x_vali_c)\n",
    "y_vali_data = Y_scaler_vali.fit_transform(y_vali_c)\n",
    "x_test_data = X_scaler_test.fit_transform(x_test_c)\n",
    "y_test_data = Y_scaler_test.fit_transform(y_test_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NOygAYEWhEHl"
   },
   "outputs": [],
   "source": [
    "def modeling1(hist_window, \n",
    "              step_out, \n",
    "              num_estimators, \n",
    "              max_depth, \n",
    "              learning_rate, \n",
    "              subsample,\n",
    "              flag_report):\n",
    "\n",
    "'''Fits models for the different desired hyperparameters and returns results\n",
    "  \n",
    "  Hyperparameters\n",
    "  \n",
    "  Data size hyperparamters:\n",
    "  hist_window: historical window size in number of weeks,\n",
    "  step_out: forecast horizon size in number of weeks,\n",
    "  \n",
    "  XGBoost-specific hyperparameters (https://xgboost.readthedocs.io/en/stable/parameter.html):\n",
    "  num_estimators\n",
    "  max_depth\n",
    "  learning_rate\n",
    "  subsample\n",
    "  '''\n",
    "\n",
    "  x_train, y_train = transform_county_data(x_train_data, y_train_data)\n",
    "  x_vali, y_vali = transform_county_data(x_vali_data, y_vali_data)\n",
    "  x_test, y_test = transform_county_data(x_test_data, y_test_data)\n",
    "\n",
    "  # Convert all the 3D arrays to 2D for XGBoost\n",
    "  \n",
    "  train_len = len(x_train)\n",
    "  num_features = X_data.shape[1]\n",
    "  vali_len = len(x_vali)\n",
    "  test_len = len(x_test)\n",
    "\n",
    "  # Reshape the labels into a simple 2D array\n",
    "  y_train = y_train.reshape(train_len, step_out)\n",
    "  y_vali = y_vali.reshape(vali_len, step_out)\n",
    "  y_test = y_test.reshape(test_len, step_out)\n",
    "\n",
    "  # Reshape the x data into a 2D array of (num windows, window size x features size)\n",
    "  x_train = x_train.reshape(train_len, hist_window * num_features)\n",
    "  x_vali = x_vali.reshape(vali_len, hist_window * num_features)\n",
    "  x_test = x_test.reshape(test_len, hist_window * num_features)\n",
    "\n",
    "  # fit the model\n",
    "  model = XGBRegressor(objective='reg:squarederror',\n",
    "                         n_estimators=num_estimators,\n",
    "                         max_depth = max_depth,\n",
    "                         learning_rate = learning_rate,\n",
    "                         subsample = subsample)\n",
    "  model.fit(x_train, y_train)\n",
    "\n",
    "  # Get predictions\n",
    "  y_test_pred = model.predict(x_test)\n",
    "    \n",
    "  # Convert predictions back onto real scale\n",
    "  y_test_pred_Inverse = Y_scaler_test.inverse_transform(y_test_pred)\n",
    "  y_test_pred_Inverse_ordinal = np.round(y_test_pred_Inverse).astype(int)\n",
    "  y_test_Inverse = Y_scaler_test.inverse_transform(y_test)\n",
    "  y_test_Inverse_ordinal = np.round(y_test_Inverse).astype(int)\n",
    "  \n",
    "  # Get evaluation mtrics\n",
    "  mse, mae = timeseries_evaluation_metrics_func(y_test_Inverse,y_test_pred_Inverse)\n",
    "\n",
    "  # Get the binary classification evaluation metrics\n",
    "  threshold = 2.5\n",
    "  y_test_Inverse_binary = np.where(y_test_Inverse >= threshold, 1, 0)\n",
    "  y_test_pred_Inverse_binary = np.where(y_test_pred_Inverse >= threshold, 1, 0)\n",
    "  f1 = timeseries_evaluation_metrics_binary(y_test_Inverse_binary,y_test_pred_Inverse_binary)\n",
    "  \n",
    "  # Print the classification report\n",
    "  if flag_report:\n",
    "    classification_metrics = classification_report(y_test_Inverse_binary.flatten(),y_test_pred_Inverse_binary.flatten())\n",
    "    print(classification_metrics)\n",
    "\n",
    "  print(f'Number of estimators: {num_estimators}, Max depth: {max_depth}, Learning rate: {learning_rate}, Subsample: {subsample}, F1: {f1}, MSE: {mse}, MAE: {mae}')\n",
    "        \n",
    "  return f1, mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.92      0.65      0.76     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.94      0.82      0.87    123888\n",
      "weighted avg       0.96      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 100, Max depth: 1, Learning rate: 0.25, Subsample: 1, F1: 0.8702138474163413, MSE: 0.3214656259878474, MAE: 0.37872495195035155\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97    110778\n",
      "           1       0.92      0.63      0.74     13110\n",
      "\n",
      "    accuracy                           0.95    123888\n",
      "   macro avg       0.94      0.81      0.86    123888\n",
      "weighted avg       0.95      0.95      0.95    123888\n",
      "\n",
      "Number of estimators: 100, Max depth: 1, Learning rate: 0.2, Subsample: 1, F1: 0.8596540430064559, MSE: 0.3193389110159124, MAE: 0.3766826262890052\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.93      0.64      0.76     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.94      0.82      0.87    123888\n",
      "weighted avg       0.96      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 100, Max depth: 1, Learning rate: 0.15, Subsample: 1, F1: 0.8661722750319141, MSE: 0.3191755564817522, MAE: 0.3754679179326797\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.93      0.63      0.75     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.94      0.81      0.86    123888\n",
      "weighted avg       0.96      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 100, Max depth: 1, Learning rate: 0.1, Subsample: 1, F1: 0.864532635819239, MSE: 0.3182870105010392, MAE: 0.3751392608152962\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.90      0.66      0.76     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.93      0.83      0.87    123888\n",
      "weighted avg       0.95      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 100, Max depth: 2, Learning rate: 0.25, Subsample: 1, F1: 0.8682558020431641, MSE: 0.2948070675394515, MAE: 0.35475461752026566\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.91      0.66      0.77     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.94      0.83      0.87    123888\n",
      "weighted avg       0.96      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 100, Max depth: 2, Learning rate: 0.2, Subsample: 1, F1: 0.8718943746096037, MSE: 0.29265230954117794, MAE: 0.35206603514876766\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.92      0.67      0.77     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.94      0.83      0.88    123888\n",
      "weighted avg       0.96      0.96      0.96    123888\n",
      "\n",
      "Number of estimators: 100, Max depth: 2, Learning rate: 0.15, Subsample: 1, F1: 0.8756821321605724, MSE: 0.2934274521542524, MAE: 0.3501191217031065\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.93      0.67      0.78     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.94      0.83      0.88    123888\n",
      "weighted avg       0.96      0.96      0.96    123888\n",
      "\n",
      "Number of estimators: 100, Max depth: 2, Learning rate: 0.1, Subsample: 1, F1: 0.8774991568164896, MSE: 0.29629836503985324, MAE: 0.3522761239030329\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.87      0.69      0.77     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.92      0.84      0.87    123888\n",
      "weighted avg       0.95      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.25, Subsample: 1, F1: 0.872960202864012, MSE: 0.32187559917972813, MAE: 0.3678626470701535\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.88      0.70      0.78     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.92      0.84      0.88    123888\n",
      "weighted avg       0.96      0.96      0.96    123888\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.2, Subsample: 1, F1: 0.8767630071245334, MSE: 0.30892191410949893, MAE: 0.3593378115660834\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.90      0.69      0.78     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.93      0.84      0.88    123888\n",
      "weighted avg       0.96      0.96      0.96    123888\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.8798821947216257, MSE: 0.29648042698318855, MAE: 0.34998273420429293\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.90      0.69      0.78     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.93      0.84      0.88    123888\n",
      "weighted avg       0.96      0.96      0.96    123888\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.1, Subsample: 1, F1: 0.879597946134863, MSE: 0.29118596947005226, MAE: 0.34488165730805814\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97    110778\n",
      "           1       0.85      0.69      0.76     13110\n",
      "\n",
      "    accuracy                           0.95    123888\n",
      "   macro avg       0.91      0.84      0.87    123888\n",
      "weighted avg       0.95      0.95      0.95    123888\n",
      "\n",
      "Number of estimators: 100, Max depth: 4, Learning rate: 0.25, Subsample: 1, F1: 0.8676045020951739, MSE: 0.33723480821701507, MAE: 0.3818649084311922\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.87      0.69      0.77     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.92      0.84      0.87    123888\n",
      "weighted avg       0.95      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 100, Max depth: 4, Learning rate: 0.2, Subsample: 1, F1: 0.8728383397304613, MSE: 0.3279724926852199, MAE: 0.3738864588217577\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.88      0.69      0.77     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.92      0.84      0.87    123888\n",
      "weighted avg       0.96      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 100, Max depth: 4, Learning rate: 0.15, Subsample: 1, F1: 0.8739488587273867, MSE: 0.3156994047811633, MAE: 0.36498373890699204\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.89      0.68      0.77     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.93      0.84      0.88    123888\n",
      "weighted avg       0.96      0.96      0.96    123888\n",
      "\n",
      "Number of estimators: 100, Max depth: 4, Learning rate: 0.1, Subsample: 1, F1: 0.8757987061824605, MSE: 0.2999035596199346, MAE: 0.354543337935796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.91      0.65      0.76     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.93      0.82      0.87    123888\n",
      "weighted avg       0.95      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 200, Max depth: 1, Learning rate: 0.25, Subsample: 1, F1: 0.8654619982808595, MSE: 0.3211511423684254, MAE: 0.3790246175825821\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97    110778\n",
      "           1       0.91      0.62      0.74     13110\n",
      "\n",
      "    accuracy                           0.95    123888\n",
      "   macro avg       0.93      0.81      0.86    123888\n",
      "weighted avg       0.95      0.95      0.95    123888\n",
      "\n",
      "Number of estimators: 200, Max depth: 1, Learning rate: 0.2, Subsample: 1, F1: 0.8569142806596455, MSE: 0.31834669814043576, MAE: 0.3763382498867828\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.92      0.64      0.75     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.94      0.81      0.86    123888\n",
      "weighted avg       0.95      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 200, Max depth: 1, Learning rate: 0.15, Subsample: 1, F1: 0.8631342979629715, MSE: 0.3173936921968013, MAE: 0.3740341430171916\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.92      0.63      0.75     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.94      0.81      0.86    123888\n",
      "weighted avg       0.95      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 200, Max depth: 1, Learning rate: 0.1, Subsample: 1, F1: 0.8636558181488223, MSE: 0.3172937913405025, MAE: 0.3734792970439531\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97    110778\n",
      "           1       0.87      0.67      0.76     13110\n",
      "\n",
      "    accuracy                           0.95    123888\n",
      "   macro avg       0.92      0.83      0.87    123888\n",
      "weighted avg       0.95      0.95      0.95    123888\n",
      "\n",
      "Number of estimators: 200, Max depth: 2, Learning rate: 0.25, Subsample: 1, F1: 0.8661825853249181, MSE: 0.31220710622665176, MAE: 0.3679101643209653\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.89      0.68      0.77     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.93      0.83      0.87    123888\n",
      "weighted avg       0.95      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 200, Max depth: 2, Learning rate: 0.2, Subsample: 1, F1: 0.8721610990295336, MSE: 0.30554238118347105, MAE: 0.3618380591253607\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.90      0.67      0.77     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.93      0.83      0.87    123888\n",
      "weighted avg       0.96      0.96      0.96    123888\n",
      "\n",
      "Number of estimators: 200, Max depth: 2, Learning rate: 0.15, Subsample: 1, F1: 0.8739808191153016, MSE: 0.2960464637637909, MAE: 0.35500601200365706\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.91      0.67      0.77     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.94      0.83      0.87    123888\n",
      "weighted avg       0.96      0.96      0.96    123888\n",
      "\n",
      "Number of estimators: 200, Max depth: 2, Learning rate: 0.1, Subsample: 1, F1: 0.8744275571265151, MSE: 0.2918639144928205, MAE: 0.35034393325895535\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97    110778\n",
      "           1       0.83      0.69      0.76     13110\n",
      "\n",
      "    accuracy                           0.95    123888\n",
      "   macro avg       0.90      0.84      0.86    123888\n",
      "weighted avg       0.95      0.95      0.95    123888\n",
      "\n",
      "Number of estimators: 200, Max depth: 3, Learning rate: 0.25, Subsample: 1, F1: 0.8649221582088356, MSE: 0.36192355984021257, MAE: 0.392888750803367\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98    110778\n",
      "           1       0.85      0.70      0.77     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.91      0.84      0.87    123888\n",
      "weighted avg       0.95      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 200, Max depth: 3, Learning rate: 0.2, Subsample: 1, F1: 0.8729224313983708, MSE: 0.33731105019324853, MAE: 0.3793322483419167\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98    110778\n",
      "           1       0.87      0.70      0.78     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.92      0.84      0.88    123888\n",
      "weighted avg       0.96      0.96      0.96    123888\n",
      "\n",
      "Number of estimators: 200, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.8762470969191966, MSE: 0.3184876724826336, MAE: 0.3655080571150098\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.89      0.70      0.78     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.93      0.84      0.88    123888\n",
      "weighted avg       0.96      0.96      0.96    123888\n",
      "\n",
      "Number of estimators: 200, Max depth: 3, Learning rate: 0.1, Subsample: 1, F1: 0.8785199151756342, MSE: 0.30268361944167765, MAE: 0.3533242934827616\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97    110778\n",
      "           1       0.82      0.68      0.75     13110\n",
      "\n",
      "    accuracy                           0.95    123888\n",
      "   macro avg       0.89      0.83      0.86    123888\n",
      "weighted avg       0.95      0.95      0.95    123888\n",
      "\n",
      "Number of estimators: 200, Max depth: 4, Learning rate: 0.25, Subsample: 1, F1: 0.8597153364266858, MSE: 0.36977500234860866, MAE: 0.4047316046700308\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97    110778\n",
      "           1       0.84      0.69      0.76     13110\n",
      "\n",
      "    accuracy                           0.95    123888\n",
      "   macro avg       0.90      0.84      0.87    123888\n",
      "weighted avg       0.95      0.95      0.95    123888\n",
      "\n",
      "Number of estimators: 200, Max depth: 4, Learning rate: 0.2, Subsample: 1, F1: 0.8653167415121051, MSE: 0.35605179199906123, MAE: 0.39347135738971967\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97    110778\n",
      "           1       0.85      0.69      0.76     13110\n",
      "\n",
      "    accuracy                           0.95    123888\n",
      "   macro avg       0.91      0.84      0.87    123888\n",
      "weighted avg       0.95      0.95      0.95    123888\n",
      "\n",
      "Number of estimators: 200, Max depth: 4, Learning rate: 0.15, Subsample: 1, F1: 0.8692194643287108, MSE: 0.34147348076855094, MAE: 0.38271952253644875\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.87      0.69      0.77     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.92      0.84      0.87    123888\n",
      "weighted avg       0.95      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 200, Max depth: 4, Learning rate: 0.1, Subsample: 1, F1: 0.8738159695270753, MSE: 0.32003098325937435, MAE: 0.3689257450412211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.89      0.66      0.76     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.93      0.82      0.87    123888\n",
      "weighted avg       0.95      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 300, Max depth: 1, Learning rate: 0.25, Subsample: 1, F1: 0.8663165495974421, MSE: 0.3256838172928569, MAE: 0.38235203334751183\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97    110778\n",
      "           1       0.90      0.63      0.74     13110\n",
      "\n",
      "    accuracy                           0.95    123888\n",
      "   macro avg       0.93      0.81      0.86    123888\n",
      "weighted avg       0.95      0.95      0.95    123888\n",
      "\n",
      "Number of estimators: 300, Max depth: 1, Learning rate: 0.2, Subsample: 1, F1: 0.8564727218609569, MSE: 0.3194224524912345, MAE: 0.3777064217649424\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.91      0.63      0.75     13110\n",
      "\n",
      "    accuracy                           0.95    123888\n",
      "   macro avg       0.94      0.81      0.86    123888\n",
      "weighted avg       0.95      0.95      0.95    123888\n",
      "\n",
      "Number of estimators: 300, Max depth: 1, Learning rate: 0.15, Subsample: 1, F1: 0.8612970902726272, MSE: 0.3170112784136913, MAE: 0.3746741490802247\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.92      0.63      0.75     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.94      0.81      0.86    123888\n",
      "weighted avg       0.95      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 300, Max depth: 1, Learning rate: 0.1, Subsample: 1, F1: 0.8632058608341583, MSE: 0.3163699739800366, MAE: 0.3731068038748481\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97    110778\n",
      "           1       0.85      0.67      0.75     13110\n",
      "\n",
      "    accuracy                           0.95    123888\n",
      "   macro avg       0.91      0.83      0.86    123888\n",
      "weighted avg       0.95      0.95      0.95    123888\n",
      "\n",
      "Number of estimators: 300, Max depth: 2, Learning rate: 0.25, Subsample: 1, F1: 0.8628947770521479, MSE: 0.3309534084246112, MAE: 0.3796105881558514\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.87      0.68      0.76     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.92      0.83      0.87    123888\n",
      "weighted avg       0.95      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 300, Max depth: 2, Learning rate: 0.2, Subsample: 1, F1: 0.8699539990540233, MSE: 0.3200275740690408, MAE: 0.37057024811953526\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.89      0.68      0.77     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.93      0.83      0.87    123888\n",
      "weighted avg       0.96      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 300, Max depth: 2, Learning rate: 0.15, Subsample: 1, F1: 0.8727957426083813, MSE: 0.3038331118482388, MAE: 0.3609553674418092\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.91      0.67      0.77     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.93      0.83      0.87    123888\n",
      "weighted avg       0.96      0.96      0.96    123888\n",
      "\n",
      "Number of estimators: 300, Max depth: 2, Learning rate: 0.1, Subsample: 1, F1: 0.8743472590666702, MSE: 0.2952351250240863, MAE: 0.3538542980727945\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97    110778\n",
      "           1       0.81      0.69      0.75     13110\n",
      "\n",
      "    accuracy                           0.95    123888\n",
      "   macro avg       0.89      0.84      0.86    123888\n",
      "weighted avg       0.95      0.95      0.95    123888\n",
      "\n",
      "Number of estimators: 300, Max depth: 3, Learning rate: 0.25, Subsample: 1, F1: 0.8607701367729306, MSE: 0.38485092443261404, MAE: 0.4078740654480018\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97    110778\n",
      "           1       0.83      0.70      0.76     13110\n",
      "\n",
      "    accuracy                           0.95    123888\n",
      "   macro avg       0.90      0.84      0.87    123888\n",
      "weighted avg       0.95      0.95      0.95    123888\n",
      "\n",
      "Number of estimators: 300, Max depth: 3, Learning rate: 0.2, Subsample: 1, F1: 0.8677375567114364, MSE: 0.36060204258278483, MAE: 0.39390018923506837\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98    110778\n",
      "           1       0.85      0.70      0.77     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.91      0.84      0.87    123888\n",
      "weighted avg       0.95      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 300, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.8723092890119382, MSE: 0.34102024329955777, MAE: 0.3803402094502835\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98    110778\n",
      "           1       0.87      0.70      0.78     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.92      0.84      0.88    123888\n",
      "weighted avg       0.96      0.96      0.96    123888\n",
      "\n",
      "Number of estimators: 300, Max depth: 3, Learning rate: 0.1, Subsample: 1, F1: 0.8769669650673504, MSE: 0.3162503805305862, MAE: 0.36345861064030793\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97    110778\n",
      "           1       0.81      0.68      0.74     13110\n",
      "\n",
      "    accuracy                           0.95    123888\n",
      "   macro avg       0.89      0.83      0.86    123888\n",
      "weighted avg       0.95      0.95      0.95    123888\n",
      "\n",
      "Number of estimators: 300, Max depth: 4, Learning rate: 0.25, Subsample: 1, F1: 0.8553053529507011, MSE: 0.3862763200034687, MAE: 0.4166197335181017\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97    110778\n",
      "           1       0.83      0.68      0.75     13110\n",
      "\n",
      "    accuracy                           0.95    123888\n",
      "   macro avg       0.89      0.83      0.86    123888\n",
      "weighted avg       0.95      0.95      0.95    123888\n",
      "\n",
      "Number of estimators: 300, Max depth: 4, Learning rate: 0.2, Subsample: 1, F1: 0.8599176133189956, MSE: 0.3722123973303027, MAE: 0.40464109967618317\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97    110778\n",
      "           1       0.84      0.69      0.76     13110\n",
      "\n",
      "    accuracy                           0.95    123888\n",
      "   macro avg       0.90      0.84      0.86    123888\n",
      "weighted avg       0.95      0.95      0.95    123888\n",
      "\n",
      "Number of estimators: 300, Max depth: 4, Learning rate: 0.15, Subsample: 1, F1: 0.864517960803786, MSE: 0.35796132777285483, MAE: 0.39471112595743374\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.86      0.70      0.77     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.91      0.84      0.87    123888\n",
      "weighted avg       0.95      0.96      0.95    123888\n",
      "\n",
      "Number of estimators: 300, Max depth: 4, Learning rate: 0.1, Subsample: 1, F1: 0.8714159696891588, MSE: 0.33437049009098313, MAE: 0.37869000085484483\n",
      "# Hist_Window   Step_Out   Num Estimators  Max Depth   Learning Rate   Subsample   F1    MSE       MAE\n",
      "30            12         100     1       0.25       1          0.8702     0.3215     0.3787    \n",
      "30            12         100     1       0.2        1          0.8597     0.3193     0.3767    \n",
      "30            12         100     1       0.15       1          0.8662     0.3192     0.3755    \n",
      "30            12         100     1       0.1        1          0.8645     0.3183     0.3751    \n",
      "30            12         100     2       0.25       1          0.8683     0.2948     0.3548    \n",
      "30            12         100     2       0.2        1          0.8719     0.2927     0.3521    \n",
      "30            12         100     2       0.15       1          0.8757     0.2934     0.3501    \n",
      "30            12         100     2       0.1        1          0.8775     0.2963     0.3523    \n",
      "30            12         100     3       0.25       1          0.8730     0.3219     0.3679    \n",
      "30            12         100     3       0.2        1          0.8768     0.3089     0.3593    \n",
      "30            12         100     3       0.15       1          0.8799     0.2965     0.3500    \n",
      "30            12         100     3       0.1        1          0.8796     0.2912     0.3449    \n",
      "30            12         100     4       0.25       1          0.8676     0.3372     0.3819    \n",
      "30            12         100     4       0.2        1          0.8728     0.3280     0.3739    \n",
      "30            12         100     4       0.15       1          0.8739     0.3157     0.3650    \n",
      "30            12         100     4       0.1        1          0.8758     0.2999     0.3545    \n",
      "30            12         200     1       0.25       1          0.8655     0.3212     0.3790    \n",
      "30            12         200     1       0.2        1          0.8569     0.3183     0.3763    \n",
      "30            12         200     1       0.15       1          0.8631     0.3174     0.3740    \n",
      "30            12         200     1       0.1        1          0.8637     0.3173     0.3735    \n",
      "30            12         200     2       0.25       1          0.8662     0.3122     0.3679    \n",
      "30            12         200     2       0.2        1          0.8722     0.3055     0.3618    \n",
      "30            12         200     2       0.15       1          0.8740     0.2960     0.3550    \n",
      "30            12         200     2       0.1        1          0.8744     0.2919     0.3503    \n",
      "30            12         200     3       0.25       1          0.8649     0.3619     0.3929    \n",
      "30            12         200     3       0.2        1          0.8729     0.3373     0.3793    \n",
      "30            12         200     3       0.15       1          0.8762     0.3185     0.3655    \n",
      "30            12         200     3       0.1        1          0.8785     0.3027     0.3533    \n",
      "30            12         200     4       0.25       1          0.8597     0.3698     0.4047    \n",
      "30            12         200     4       0.2        1          0.8653     0.3561     0.3935    \n",
      "30            12         200     4       0.15       1          0.8692     0.3415     0.3827    \n",
      "30            12         200     4       0.1        1          0.8738     0.3200     0.3689    \n",
      "30            12         300     1       0.25       1          0.8663     0.3257     0.3824    \n",
      "30            12         300     1       0.2        1          0.8565     0.3194     0.3777    \n",
      "30            12         300     1       0.15       1          0.8613     0.3170     0.3747    \n",
      "30            12         300     1       0.1        1          0.8632     0.3164     0.3731    \n",
      "30            12         300     2       0.25       1          0.8629     0.3310     0.3796    \n",
      "30            12         300     2       0.2        1          0.8700     0.3200     0.3706    \n",
      "30            12         300     2       0.15       1          0.8728     0.3038     0.3610    \n",
      "30            12         300     2       0.1        1          0.8743     0.2952     0.3539    \n",
      "30            12         300     3       0.25       1          0.8608     0.3849     0.4079    \n",
      "30            12         300     3       0.2        1          0.8677     0.3606     0.3939    \n",
      "30            12         300     3       0.15       1          0.8723     0.3410     0.3803    \n",
      "30            12         300     3       0.1        1          0.8770     0.3163     0.3635    \n",
      "30            12         300     4       0.25       1          0.8553     0.3863     0.4166    \n",
      "30            12         300     4       0.2        1          0.8599     0.3722     0.4046    \n",
      "30            12         300     4       0.15       1          0.8645     0.3580     0.3947    \n",
      "30            12         300     4       0.1        1          0.8714     0.3344     0.3787    \n"
     ]
    }
   ],
   "source": [
    "# Set the window and horizon size for model parameter testing\n",
    "parameter_result_list1 = []\n",
    "hist_window = 30\n",
    "step_out = 12\n",
    "subsample = 1\n",
    "\n",
    "for num_estimators in [100,200,300]:\n",
    "    for max_depth in [1,2,3,4]:\n",
    "        for learning_rate in [0.25, 0.2, 0.15, 0.1]:\n",
    "            f1, mse, mae = modeling1(hist_window, \n",
    "                             step_out, \n",
    "                             num_estimators, \n",
    "                             max_depth, \n",
    "                             learning_rate, \n",
    "                             subsample = 1, \n",
    "                             flag_report = 1)\n",
    "            parameter_result_list1.append((hist_window, \n",
    "                                   step_out, \n",
    "                                   num_estimators,\n",
    "                                   max_depth,\n",
    "                                   learning_rate,\n",
    "                                   subsample, \n",
    "                                   f1, mse, mae))\n",
    "\n",
    "# Printing the list with comment lines indicating parameter titles\n",
    "print(\"# Hist_Window   Step_Out   Num Estimators  Max Depth   Learning Rate   Subsample   F1    MSE       MAE\")\n",
    "for params in parameter_result_list1:\n",
    "    print(\"{:<13} {:<10} {:<7} {:<7} {:<10} {:<10} {:<10.4f} {:<10.4f} {:<10.4f}\".format(*params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ivpbVeCFIqdJ"
   },
   "outputs": [],
   "source": [
    "# Saving the parameter_result_list to a CSV file\n",
    "with open('data/xgboost_parameter_result_list1.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(parameter_result_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "JogzhJEsAzTP"
   },
   "outputs": [],
   "source": [
    "# Loading the parameter_result_list from the CSV file\n",
    "parameter_result_list = []\n",
    "\n",
    "with open('data/xgboost_parameter_result_list1.csv', 'r', newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        parameter_result_list.append(row)\n",
    "#parameter_result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLxJO_CvTu8q",
    "outputId": "c58d7074-7a47-4197-836f-bc71c653c622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 12, 100, 3, 0.15, 1, 0.8798821947216257, 0.29648042698318855, 0.34998273420429293]\n",
      "[30, 12, 100, 3, 0.1, 1, 0.879597946134863, 0.29118596947005226, 0.34488165730805814]\n",
      "[30, 12, 200, 3, 0.1, 1, 0.8785199151756342, 0.30268361944167765, 0.3533242934827616]\n",
      "[30, 12, 100, 2, 0.1, 1, 0.8774991568164896, 0.29629836503985324, 0.3522761239030329]\n",
      "[30, 12, 300, 3, 0.1, 1, 0.8769669650673504, 0.3162503805305862, 0.36345861064030793]\n",
      "[30, 12, 100, 3, 0.2, 1, 0.8767630071245334, 0.30892191410949893, 0.3593378115660834]\n",
      "[30, 12, 200, 3, 0.15, 1, 0.8762470969191966, 0.3184876724826336, 0.3655080571150098]\n",
      "[30, 12, 100, 4, 0.1, 1, 0.8757987061824605, 0.2999035596199346, 0.354543337935796]\n",
      "[30, 12, 100, 2, 0.15, 1, 0.8756821321605724, 0.2934274521542524, 0.3501191217031065]\n",
      "[30, 12, 200, 2, 0.1, 1, 0.8744275571265151, 0.2918639144928205, 0.35034393325895535]\n",
      "[30, 12, 300, 2, 0.1, 1, 0.8743472590666702, 0.2952351250240863, 0.3538542980727945]\n",
      "[30, 12, 200, 2, 0.15, 1, 0.8739808191153016, 0.2960464637637909, 0.35500601200365706]\n",
      "[30, 12, 100, 4, 0.15, 1, 0.8739488587273867, 0.3156994047811633, 0.36498373890699204]\n",
      "[30, 12, 200, 4, 0.1, 1, 0.8738159695270753, 0.32003098325937435, 0.3689257450412211]\n",
      "[30, 12, 100, 3, 0.25, 1, 0.872960202864012, 0.32187559917972813, 0.3678626470701535]\n",
      "[30, 12, 200, 3, 0.2, 1, 0.8729224313983708, 0.33731105019324853, 0.3793322483419167]\n",
      "[30, 12, 100, 4, 0.2, 1, 0.8728383397304613, 0.3279724926852199, 0.3738864588217577]\n",
      "[30, 12, 300, 2, 0.15, 1, 0.8727957426083813, 0.3038331118482388, 0.3609553674418092]\n",
      "[30, 12, 300, 3, 0.15, 1, 0.8723092890119382, 0.34102024329955777, 0.3803402094502835]\n",
      "[30, 12, 200, 2, 0.2, 1, 0.8721610990295336, 0.30554238118347105, 0.3618380591253607]\n",
      "[30, 12, 100, 2, 0.2, 1, 0.8718943746096037, 0.29265230954117794, 0.35206603514876766]\n",
      "[30, 12, 300, 4, 0.1, 1, 0.8714159696891588, 0.33437049009098313, 0.37869000085484483]\n",
      "[30, 12, 100, 1, 0.25, 1, 0.8702138474163413, 0.3214656259878474, 0.37872495195035155]\n",
      "[30, 12, 300, 2, 0.2, 1, 0.8699539990540233, 0.3200275740690408, 0.37057024811953526]\n",
      "[30, 12, 200, 4, 0.15, 1, 0.8692194643287108, 0.34147348076855094, 0.38271952253644875]\n",
      "[30, 12, 100, 2, 0.25, 1, 0.8682558020431641, 0.2948070675394515, 0.35475461752026566]\n",
      "[30, 12, 300, 3, 0.2, 1, 0.8677375567114364, 0.36060204258278483, 0.39390018923506837]\n",
      "[30, 12, 100, 4, 0.25, 1, 0.8676045020951739, 0.33723480821701507, 0.3818649084311922]\n",
      "[30, 12, 300, 1, 0.25, 1, 0.8663165495974421, 0.3256838172928569, 0.38235203334751183]\n",
      "[30, 12, 200, 2, 0.25, 1, 0.8661825853249181, 0.31220710622665176, 0.3679101643209653]\n",
      "[30, 12, 100, 1, 0.15, 1, 0.8661722750319141, 0.3191755564817522, 0.3754679179326797]\n",
      "[30, 12, 200, 1, 0.25, 1, 0.8654619982808595, 0.3211511423684254, 0.3790246175825821]\n",
      "[30, 12, 200, 4, 0.2, 1, 0.8653167415121051, 0.35605179199906123, 0.39347135738971967]\n",
      "[30, 12, 200, 3, 0.25, 1, 0.8649221582088356, 0.36192355984021257, 0.392888750803367]\n",
      "[30, 12, 100, 1, 0.1, 1, 0.864532635819239, 0.3182870105010392, 0.3751392608152962]\n",
      "[30, 12, 300, 4, 0.15, 1, 0.864517960803786, 0.35796132777285483, 0.39471112595743374]\n",
      "[30, 12, 200, 1, 0.1, 1, 0.8636558181488223, 0.3172937913405025, 0.3734792970439531]\n",
      "[30, 12, 300, 1, 0.1, 1, 0.8632058608341583, 0.3163699739800366, 0.3731068038748481]\n",
      "[30, 12, 200, 1, 0.15, 1, 0.8631342979629715, 0.3173936921968013, 0.3740341430171916]\n",
      "[30, 12, 300, 2, 0.25, 1, 0.8628947770521479, 0.3309534084246112, 0.3796105881558514]\n",
      "[30, 12, 300, 1, 0.15, 1, 0.8612970902726272, 0.3170112784136913, 0.3746741490802247]\n",
      "[30, 12, 300, 3, 0.25, 1, 0.8607701367729306, 0.38485092443261404, 0.4078740654480018]\n",
      "[30, 12, 300, 4, 0.2, 1, 0.8599176133189956, 0.3722123973303027, 0.40464109967618317]\n",
      "[30, 12, 200, 4, 0.25, 1, 0.8597153364266858, 0.36977500234860866, 0.4047316046700308]\n",
      "[30, 12, 100, 1, 0.2, 1, 0.8596540430064559, 0.3193389110159124, 0.3766826262890052]\n",
      "[30, 12, 200, 1, 0.2, 1, 0.8569142806596455, 0.31834669814043576, 0.3763382498867828]\n",
      "[30, 12, 300, 1, 0.2, 1, 0.8564727218609569, 0.3194224524912345, 0.3777064217649424]\n",
      "[30, 12, 300, 4, 0.25, 1, 0.8553053529507011, 0.3862763200034687, 0.4166197335181017]\n"
     ]
    }
   ],
   "source": [
    "# Convert all elements in the list to float\n",
    "my_list_float = [[float(val) if '.' in val else int(val) for val in sublist] for sublist in parameter_result_list]\n",
    "\n",
    "# Sort the list based on the f1 values\n",
    "sorted_list = sorted(my_list_float, key=lambda x: x[6], reverse=True)\n",
    "\n",
    "# Print the sorted list\n",
    "for sublist in sorted_list:\n",
    "    print(sublist)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
