{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oeCE85RZaBN0"
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler, StandardScaler\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import datetime\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4dmgFLz8_ySQ"
   },
   "outputs": [],
   "source": [
    "# Set random seed for TensorFlow\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# Set random seed for Python\n",
    "np.random.seed(123)\n",
    "\n",
    "tf.keras.utils.set_random_seed(1)\n",
    "\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_1Ua7mbsbUOr"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_all_county = pd.read_csv('data/CA_data_lat_log_weekly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date to datetime to extract the month value\n",
    "data_all_county['date'] = pd.to_datetime(data_all_county['date'])\n",
    "data_all_county['month'] = data_all_county['date'].dt.month\n",
    "data_all_county['month'] = data_all_county['month'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkZqApXYXb-9"
   },
   "source": [
    "# Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fIDCIpwSXXKt"
   },
   "outputs": [],
   "source": [
    "def ts_multi_data_prep(dataset, target, start, end, window, step_out):\n",
    "    '''Slices the dataset into sliding and overlapping windows of desired window size and forecast horizon'''\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    start = start + window\n",
    "    if end is None:\n",
    "        end = len(dataset) - step_out\n",
    "        #end = len(dataset)\n",
    "    for i in range(start, end):\n",
    "        indices = range(i-window, i)\n",
    "        X.append(dataset[indices])\n",
    "\n",
    "        indicey = range(i, i+step_out) #revise the window definition\n",
    "        y.append(target[indicey])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vp6wtluEXfZb"
   },
   "outputs": [],
   "source": [
    "def timeseries_evaluation_metrics_func(y_true, y_pred):\n",
    "    '''Calculates and returns the evaluation metrics, MSE and MAE'''\n",
    "\n",
    "    print('Evaluation metric results:-')\n",
    "    mse = metrics.mean_squared_error(y_true.flatten(), y_pred.flatten())\n",
    "    mae = metrics.mean_absolute_error(y_true.flatten(), y_pred.flatten())\n",
    "    \n",
    "    # Can calculate RMSE and R2 and print results if desired\n",
    "    #rmse = np.sqrt(mse)\n",
    "    #r2 = metrics.r2_score(y_true.flatten(), y_pred.flatten())\n",
    "    #print(f'MSE is : {mse}')\n",
    "    #print(f'MAE is : {mae}')\n",
    "    #print(f'RMSE is : {rmse}')\n",
    "    #print(f'R2 is : {r2}\\n')\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DZJVpSJ_49m2"
   },
   "outputs": [],
   "source": [
    "def timeseries_evaluation_metrics_binary(y_true, y_pred):\n",
    "    '''Calculates and returns the F1 score for the binary classification task'''\n",
    "    \n",
    "    print('Evaluation metric results:-')\n",
    "    accuracy = accuracy_score(y_true.flatten(), y_pred.flatten())\n",
    "    precision = precision_score(y_true.flatten(), y_pred.flatten(), average='macro')\n",
    "    recall = recall_score(y_true.flatten(), y_pred.flatten(), average='macro')\n",
    "    f1 = f1_score(y_true.flatten(), y_pred.flatten(), average='macro')\n",
    "\n",
    "    #print(f'Accuracy: {accuracy}')\n",
    "    #print(f'Precision: {precision}')\n",
    "    #print(f'Recall: {recall}')\n",
    "    #print(f'F1-score: {f1}\\n')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xxvHTDdUqxyN"
   },
   "outputs": [],
   "source": [
    "def transform_county_data(x_data_array, y_data_array):\n",
    "    # Lists to store x_train_c and y_train_c arrays\n",
    "    x_c_list = []\n",
    "    y_c_list = []\n",
    "    \n",
    "    # Divide the arrays into 'unique_fips_count' number of subarrays\n",
    "    x_subarrays = np.array_split(x_data_array, unique_fips_count, axis=0)\n",
    "    y_subarrays = np.array_split(y_data_array, unique_fips_count, axis=0)\n",
    "\n",
    "    # Combine x_subarrays and y_subarrays into tuples\n",
    "    data_tuples = [(x_subarray, y_subarray) for x_subarray, y_subarray in zip(x_subarrays, y_subarrays)]\n",
    "\n",
    "    # Print or use the data tuples as needed\n",
    "    for idx, data_tuple in enumerate(data_tuples):\n",
    "        x_window_c, y_window_c = ts_multi_data_prep(data_tuple[0],data_tuple[1], 0, None, hist_window, step_out)\n",
    "        # Append x_window_c and y_window_c arrays to lists\n",
    "        x_c_list.append(x_window_c)\n",
    "        y_c_list.append(y_window_c)\n",
    "\n",
    "    # Stack arrays in lists to create x_train_c and y_train_c\n",
    "    x_all_county = np.vstack(x_c_list)\n",
    "    y_all_county = np.vstack(y_c_list)\n",
    "\n",
    "    return x_all_county, y_all_county"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BJyFdGZFeeVi"
   },
   "outputs": [],
   "source": [
    "# Create empty lists to hold arrays\n",
    "x_train_c, y_train_c, x_vali_c, y_vali_c, x_test_c, y_test_c = [], [], [], [], [], []\n",
    "\n",
    "# Get the unique FIPS or county codes\n",
    "unique_fips = data_all_county['fips'].unique()\n",
    "unique_fips_count = data_all_county['fips'].nunique()\n",
    "\n",
    "for fips in unique_fips:\n",
    "    # Extract dataframe for the current FIPS value\n",
    "    data_county = data_all_county[data_all_county['fips'] == fips]\n",
    "\n",
    "    # Select features to use in the dataset\n",
    "    X_data = data_county[['lat','lon','PRECTOT', 'PS', 'QV2M', 'T2M', 'T2MDEW', 'T2MWET',\n",
    "       'T2M_MAX', 'T2M_MIN', 'T2M_RANGE', 'TS', 'WS10M', 'WS10M_MAX',\n",
    "       'WS10M_MIN', 'WS10M_RANGE', 'WS50M', 'WS50M_MAX', 'WS50M_MIN',\n",
    "       'WS50M_RANGE', 'score', 'month']]\n",
    "    \n",
    "    # Set the labels\n",
    "    Y_data = data_county[['score']]\n",
    "    \n",
    "    #train_val_test split 70%-10%-20%\n",
    "    n = len(X_data)\n",
    "\n",
    "    x_train_county = X_data[0:int(n*0.7)]\n",
    "    y_train_county = Y_data[0:int(n*0.7)]\n",
    "    x_vali_county = X_data[int(n*0.7):int(n*0.8)]\n",
    "    y_vali_county = Y_data[int(n*0.7):int(n*0.8)]\n",
    "    x_test_county = X_data[int(n*0.8):]\n",
    "    y_test_county = Y_data[int(n*0.8):]\n",
    "\n",
    "    # Condition for the first county in CA to start populating the lists\n",
    "    if fips == 6001:\n",
    "        x_train_c, y_train_c, x_vali_c, y_vali_c, x_test_c, y_test_c = x_train_county, y_train_county, x_vali_county, y_vali_county, x_test_county, y_test_county\n",
    "\n",
    "    # Append the following counties to the lists\n",
    "    else:\n",
    "        x_train_c = np.concatenate((x_train_c, x_train_county), axis=0)\n",
    "        y_train_c = np.concatenate((y_train_c, y_train_county), axis=0)\n",
    "        x_vali_c = np.concatenate((x_vali_c, x_vali_county), axis=0)\n",
    "        y_vali_c = np.concatenate((y_vali_c, y_vali_county), axis=0)\n",
    "        x_test_c = np.concatenate((x_test_c, x_test_county), axis=0)\n",
    "        y_test_c = np.concatenate((y_test_c, y_test_county), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MjYfAZXoYPE7"
   },
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_scaler_train = MinMaxScaler()\n",
    "Y_scaler_train = MinMaxScaler()\n",
    "X_scaler_test = MinMaxScaler()\n",
    "Y_scaler_test = MinMaxScaler()\n",
    "X_scaler_vali = MinMaxScaler()\n",
    "Y_scaler_vali = MinMaxScaler()\n",
    "x_train_data = X_scaler_train.fit_transform(x_train_c)\n",
    "y_train_data = Y_scaler_train.fit_transform(y_train_c)\n",
    "x_vali_data = X_scaler_vali.fit_transform(x_vali_c)\n",
    "y_vali_data = Y_scaler_vali.fit_transform(y_vali_c)\n",
    "x_test_data = X_scaler_test.fit_transform(x_test_c)\n",
    "y_test_data = Y_scaler_test.fit_transform(y_test_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NOygAYEWhEHl"
   },
   "outputs": [],
   "source": [
    "def modeling(hist_window, \n",
    "              step_out, \n",
    "              num_estimators, \n",
    "              max_depth, \n",
    "              learning_rate, \n",
    "              subsample,\n",
    "              flag_report):\n",
    "  '''Fits models for the different desired hyperparameters and returns results\n",
    "  \n",
    "  Hyperparameters\n",
    "  \n",
    "  Data size hyperparamters:\n",
    "  hist_window: historical window size in number of weeks,\n",
    "  step_out: forecast horizon size in number of weeks,\n",
    "  \n",
    "  XGBoost-specific hyperparameters (https://xgboost.readthedocs.io/en/stable/parameter.html):\n",
    "  num_estimators\n",
    "  max_depth\n",
    "  learning_rate\n",
    "  subsample\n",
    "  '''\n",
    "\n",
    "  x_train, y_train = transform_county_data(x_train_data, y_train_data)\n",
    "  x_vali, y_vali = transform_county_data(x_vali_data, y_vali_data)\n",
    "  x_test, y_test = transform_county_data(x_test_data, y_test_data)\n",
    "\n",
    "  # Convert all the 3D arrays to 2D for XGBoost\n",
    "  \n",
    "  train_len = len(x_train)\n",
    "  num_features = X_data.shape[1]\n",
    "  vali_len = len(x_vali)\n",
    "  test_len = len(x_test)\n",
    "\n",
    "  # Reshape the labels into a simple 2D array\n",
    "  y_train = y_train.reshape(train_len, step_out)\n",
    "  y_vali = y_vali.reshape(vali_len, step_out)\n",
    "  y_test = y_test.reshape(test_len, step_out)\n",
    "\n",
    "  # Reshape the x data into a 2D array of (num windows, window size x features size)\n",
    "  x_train = x_train.reshape(train_len, hist_window * num_features)\n",
    "  x_vali = x_vali.reshape(vali_len, hist_window * num_features)\n",
    "  x_test = x_test.reshape(test_len, hist_window * num_features)\n",
    "\n",
    "  # fit the model\n",
    "  model = XGBRegressor(objective='reg:squarederror',\n",
    "                         n_estimators=num_estimators,\n",
    "                         max_depth = max_depth,\n",
    "                         learning_rate = learning_rate,\n",
    "                         subsample = subsample)\n",
    "  model.fit(x_train, y_train)\n",
    "\n",
    "  # Get predictions\n",
    "  y_test_pred = model.predict(x_test)\n",
    "    \n",
    "  # Convert predictions back onto real scale\n",
    "  y_test_pred_Inverse = Y_scaler_test.inverse_transform(y_test_pred)\n",
    "  y_test_pred_Inverse_ordinal = np.round(y_test_pred_Inverse).astype(int)\n",
    "  y_test_Inverse = Y_scaler_test.inverse_transform(y_test)\n",
    "  y_test_Inverse_ordinal = np.round(y_test_Inverse).astype(int)\n",
    "  \n",
    "  # Get evaluation mtrics\n",
    "  mse, mae = timeseries_evaluation_metrics_func(y_test_Inverse,y_test_pred_Inverse)\n",
    "\n",
    "  # Get the binary classification evaluation metrics\n",
    "  threshold = 2.5\n",
    "  y_test_Inverse_binary = np.where(y_test_Inverse >= threshold, 1, 0)\n",
    "  y_test_pred_Inverse_binary = np.where(y_test_pred_Inverse >= threshold, 1, 0)\n",
    "  f1 = timeseries_evaluation_metrics_binary(y_test_Inverse_binary,y_test_pred_Inverse_binary)\n",
    "  \n",
    "  # Print the classification report\n",
    "  if flag_report:\n",
    "    classification_metrics = classification_report(y_test_Inverse_binary.flatten(),y_test_pred_Inverse_binary.flatten())\n",
    "    print(classification_metrics)\n",
    "\n",
    "  print(f'Number of estimators: {num_estimators}, Max depth: {max_depth}, Learning rate: {learning_rate}, Subsample: {subsample}, F1: {f1}, MSE: {mse}, MAE: {mae}')\n",
    "        \n",
    "  return f1, mse, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window and Horizon Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     42235\n",
      "           1       0.95      0.87      0.90      5093\n",
      "\n",
      "    accuracy                           0.98     47328\n",
      "   macro avg       0.96      0.93      0.95     47328\n",
      "weighted avg       0.98      0.98      0.98     47328\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.9462788459378368, MSE: 0.11963473901034674, MAE: 0.19024876113492895\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     83346\n",
      "           1       0.91      0.79      0.84      9454\n",
      "\n",
      "    accuracy                           0.97     92800\n",
      "   macro avg       0.94      0.89      0.91     92800\n",
      "weighted avg       0.97      0.97      0.97     92800\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.9130429332596823, MSE: 0.20701634843496627, MAE: 0.2721204877408845\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98    123100\n",
      "           1       0.88      0.70      0.78     13316\n",
      "\n",
      "    accuracy                           0.96    136416\n",
      "   macro avg       0.92      0.85      0.88    136416\n",
      "weighted avg       0.96      0.96      0.96    136416\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.8793077719430614, MSE: 0.2842063915213157, MAE: 0.33429999430794255\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97    161459\n",
      "           1       0.83      0.63      0.72     16717\n",
      "\n",
      "    accuracy                           0.95    178176\n",
      "   macro avg       0.90      0.81      0.85    178176\n",
      "weighted avg       0.95      0.95      0.95    178176\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.8456995436301329, MSE: 0.35543638575173847, MAE: 0.385947207178316\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     39626\n",
      "           1       0.97      0.86      0.91      4918\n",
      "\n",
      "    accuracy                           0.98     44544\n",
      "   macro avg       0.98      0.93      0.95     44544\n",
      "weighted avg       0.98      0.98      0.98     44544\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.951837600480699, MSE: 0.11209263736576469, MAE: 0.18481798965605795\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     77984\n",
      "           1       0.93      0.78      0.85      9248\n",
      "\n",
      "    accuracy                           0.97     87232\n",
      "   macro avg       0.95      0.89      0.92     87232\n",
      "weighted avg       0.97      0.97      0.97     87232\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.9155806911852923, MSE: 0.19760611870478384, MAE: 0.2698320964064132\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98    114954\n",
      "           1       0.89      0.69      0.78     13110\n",
      "\n",
      "    accuracy                           0.96    128064\n",
      "   macro avg       0.93      0.84      0.88    128064\n",
      "weighted avg       0.96      0.96      0.96    128064\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.8788421058553156, MSE: 0.28480287292168577, MAE: 0.33993524260793\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97    150529\n",
      "           1       0.85      0.61      0.71     16511\n",
      "\n",
      "    accuracy                           0.95    167040\n",
      "   macro avg       0.91      0.80      0.84    167040\n",
      "weighted avg       0.95      0.95      0.95    167040\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.8413510843615499, MSE: 0.3663516375815304, MAE: 0.3973804000426645\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     38234\n",
      "           1       0.97      0.87      0.91      4918\n",
      "\n",
      "    accuracy                           0.98     43152\n",
      "   macro avg       0.98      0.93      0.95     43152\n",
      "weighted avg       0.98      0.98      0.98     43152\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.9522481063957986, MSE: 0.11713551367781273, MAE: 0.19334002834694375\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     75200\n",
      "           1       0.93      0.78      0.85      9248\n",
      "\n",
      "    accuracy                           0.97     84448\n",
      "   macro avg       0.95      0.88      0.91     84448\n",
      "weighted avg       0.97      0.97      0.97     84448\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.9149312392328789, MSE: 0.20596715650900713, MAE: 0.2783315632975677\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    110778\n",
      "           1       0.90      0.69      0.78     13110\n",
      "\n",
      "    accuracy                           0.96    123888\n",
      "   macro avg       0.93      0.84      0.88    123888\n",
      "weighted avg       0.96      0.96      0.96    123888\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.8798821947216257, MSE: 0.29648042698318855, MAE: 0.34998273420429293\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97    144961\n",
      "           1       0.85      0.61      0.71     16511\n",
      "\n",
      "    accuracy                           0.95    161472\n",
      "   macro avg       0.90      0.80      0.84    161472\n",
      "weighted avg       0.95      0.95      0.95    161472\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.8414979747114353, MSE: 0.3833417840058648, MAE: 0.4091780035166267\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     35914\n",
      "           1       0.97      0.87      0.91      4918\n",
      "\n",
      "    accuracy                           0.98     40832\n",
      "   macro avg       0.98      0.93      0.95     40832\n",
      "weighted avg       0.98      0.98      0.98     40832\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.9519247506737096, MSE: 0.12409801680637325, MAE: 0.20278282904570416\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     70560\n",
      "           1       0.93      0.79      0.85      9248\n",
      "\n",
      "    accuracy                           0.97     79808\n",
      "   macro avg       0.95      0.89      0.92     79808\n",
      "weighted avg       0.97      0.97      0.97     79808\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.9176186002049829, MSE: 0.22173729772208944, MAE: 0.2966628010965734\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    103818\n",
      "           1       0.89      0.70      0.78     13110\n",
      "\n",
      "    accuracy                           0.96    116928\n",
      "   macro avg       0.93      0.84      0.88    116928\n",
      "weighted avg       0.95      0.96      0.95    116928\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.8792487459430098, MSE: 0.3160854235391638, MAE: 0.3704211257106233\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97    135681\n",
      "           1       0.85      0.62      0.72     16511\n",
      "\n",
      "    accuracy                           0.95    152192\n",
      "   macro avg       0.90      0.80      0.84    152192\n",
      "weighted avg       0.94      0.95      0.94    152192\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.8428571990768492, MSE: 0.4079094946312232, MAE: 0.43514416950543683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     33130\n",
      "           1       0.96      0.87      0.91      4918\n",
      "\n",
      "    accuracy                           0.98     38048\n",
      "   macro avg       0.97      0.93      0.95     38048\n",
      "weighted avg       0.98      0.98      0.98     38048\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.9505621398063535, MSE: 0.13103109724576678, MAE: 0.21603762607931296\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     64992\n",
      "           1       0.93      0.79      0.85      9248\n",
      "\n",
      "    accuracy                           0.97     74240\n",
      "   macro avg       0.95      0.89      0.92     74240\n",
      "weighted avg       0.96      0.97      0.96     74240\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.9159672055598698, MSE: 0.23228815059856325, MAE: 0.31391418720822206\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97     95466\n",
      "           1       0.89      0.70      0.78     13110\n",
      "\n",
      "    accuracy                           0.95    108576\n",
      "   macro avg       0.92      0.84      0.88    108576\n",
      "weighted avg       0.95      0.95      0.95    108576\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.8787307513781377, MSE: 0.3329440244755912, MAE: 0.3890546850556872\n",
      "Evaluation metric results:-\n",
      "Evaluation metric results:-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97    124545\n",
      "           1       0.85      0.63      0.72     16511\n",
      "\n",
      "    accuracy                           0.94    141056\n",
      "   macro avg       0.90      0.81      0.85    141056\n",
      "weighted avg       0.94      0.94      0.94    141056\n",
      "\n",
      "Number of estimators: 100, Max depth: 3, Learning rate: 0.15, Subsample: 1, F1: 0.845158919387305, MSE: 0.43076147513101565, MAE: 0.4503070462963061\n",
      "# Hist_Window   Step_Out   Num Estimators  Max Depth   Learning Rate   Subsample   F1    MSE       MAE\n",
      "12            4          100     3       0.15       1          0.9463     0.1196     0.1902    \n",
      "12            8          100     3       0.15       1          0.9130     0.2070     0.2721    \n",
      "12            12         100     3       0.15       1          0.8793     0.2842     0.3343    \n",
      "12            16         100     3       0.15       1          0.8457     0.3554     0.3859    \n",
      "24            4          100     3       0.15       1          0.9518     0.1121     0.1848    \n",
      "24            8          100     3       0.15       1          0.9156     0.1976     0.2698    \n",
      "24            12         100     3       0.15       1          0.8788     0.2848     0.3399    \n",
      "24            16         100     3       0.15       1          0.8414     0.3664     0.3974    \n",
      "30            4          100     3       0.15       1          0.9522     0.1171     0.1933    \n",
      "30            8          100     3       0.15       1          0.9149     0.2060     0.2783    \n",
      "30            12         100     3       0.15       1          0.8799     0.2965     0.3500    \n",
      "30            16         100     3       0.15       1          0.8415     0.3833     0.4092    \n",
      "40            4          100     3       0.15       1          0.9519     0.1241     0.2028    \n",
      "40            8          100     3       0.15       1          0.9176     0.2217     0.2967    \n",
      "40            12         100     3       0.15       1          0.8792     0.3161     0.3704    \n",
      "40            16         100     3       0.15       1          0.8429     0.4079     0.4351    \n",
      "52            4          100     3       0.15       1          0.9506     0.1310     0.2160    \n",
      "52            8          100     3       0.15       1          0.9160     0.2323     0.3139    \n",
      "52            12         100     3       0.15       1          0.8787     0.3329     0.3891    \n",
      "52            16         100     3       0.15       1          0.8452     0.4308     0.4503    \n"
     ]
    }
   ],
   "source": [
    "parameter_result_list_window = []\n",
    "\n",
    "# Set the desired XGBoost hyperparameters\n",
    "num_estimators = 100\n",
    "max_depth = 3\n",
    "learning_rate = 0.15\n",
    "subsample = 1\n",
    "\n",
    "# Set initial values for window and horizon sizes\n",
    "hist_window = 30\n",
    "step_out = 12\n",
    "\n",
    "# Loop through desired window and horizon sizes to compare results\n",
    "for hist_window in [12, 24, 30, 40, 52]:\n",
    "    for step_out in [4, 8, 12, 16]:\n",
    "        f1, mse, mae = modeling(hist_window, \n",
    "                             step_out, \n",
    "                             num_estimators = num_estimators, \n",
    "                             max_depth = max_depth, \n",
    "                             learning_rate = learning_rate, \n",
    "                             subsample = 1, \n",
    "                             flag_report = 1)\n",
    "        parameter_result_list_window.append((hist_window, \n",
    "                                   step_out, \n",
    "                                   num_estimators,\n",
    "                                   max_depth,\n",
    "                                   learning_rate,\n",
    "                                   subsample, \n",
    "                                   f1, mse, mae))\n",
    "\n",
    "\n",
    "# Printing the list with comment lines indicating parameter titles\n",
    "print(\"# Hist_Window   Step_Out   Num Estimators  Max Depth   Learning Rate   Subsample   F1    MSE       MAE\")\n",
    "\n",
    "for params in parameter_result_list_window:\n",
    "    print(\"{:<13} {:<10} {:<7} {:<7} {:<10} {:<10} {:<10.4f} {:<10.4f} {:<10.4f}\".format(*params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ivpbVeCFIqdJ"
   },
   "outputs": [],
   "source": [
    "# Save the parameter_result_list to a CSV file\n",
    "with open('data/xgboost_parameter_result_list_windows.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(parameter_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JogzhJEsAzTP"
   },
   "outputs": [],
   "source": [
    "# Load the parameter_result_list from the CSV file\n",
    "parameter_result_list = []\n",
    "\n",
    "with open('data/xgboost_parameter_result_list_windows.csv', 'r', newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        parameter_result_list.append(row)\n",
    "#parameter_result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLxJO_CvTu8q",
    "outputId": "c58d7074-7a47-4197-836f-bc71c653c622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 4, 100, 3, 0.15, 1, 0.9522481063957986, 0.11713551367781273, 0.19334002834694375]\n",
      "[40, 4, 100, 3, 0.15, 1, 0.9519247506737096, 0.12409801680637325, 0.20278282904570416]\n",
      "[24, 4, 100, 3, 0.15, 1, 0.951837600480699, 0.11209263736576469, 0.18481798965605795]\n",
      "[52, 4, 100, 3, 0.15, 1, 0.9505621398063535, 0.13103109724576678, 0.21603762607931296]\n",
      "[12, 4, 100, 3, 0.15, 1, 0.9462788459378368, 0.11963473901034674, 0.19024876113492895]\n",
      "[40, 8, 100, 3, 0.15, 1, 0.9176186002049829, 0.22173729772208944, 0.2966628010965734]\n",
      "[52, 8, 100, 3, 0.15, 1, 0.9159672055598698, 0.23228815059856325, 0.31391418720822206]\n",
      "[24, 8, 100, 3, 0.15, 1, 0.9155806911852923, 0.19760611870478384, 0.2698320964064132]\n",
      "[30, 8, 100, 3, 0.15, 1, 0.9149312392328789, 0.20596715650900713, 0.2783315632975677]\n",
      "[12, 8, 100, 3, 0.15, 1, 0.9130429332596823, 0.20701634843496627, 0.2721204877408845]\n",
      "[30, 12, 100, 3, 0.15, 1, 0.8798821947216257, 0.29648042698318855, 0.34998273420429293]\n",
      "[12, 12, 100, 3, 0.15, 1, 0.8793077719430614, 0.2842063915213157, 0.33429999430794255]\n",
      "[40, 12, 100, 3, 0.15, 1, 0.8792487459430098, 0.3160854235391638, 0.3704211257106233]\n",
      "[24, 12, 100, 3, 0.15, 1, 0.8788421058553156, 0.28480287292168577, 0.33993524260793]\n",
      "[52, 12, 100, 3, 0.15, 1, 0.8787307513781377, 0.3329440244755912, 0.3890546850556872]\n",
      "[12, 16, 100, 3, 0.15, 1, 0.8456995436301329, 0.35543638575173847, 0.385947207178316]\n",
      "[52, 16, 100, 3, 0.15, 1, 0.845158919387305, 0.43076147513101565, 0.4503070462963061]\n",
      "[40, 16, 100, 3, 0.15, 1, 0.8428571990768492, 0.4079094946312232, 0.43514416950543683]\n",
      "[30, 16, 100, 3, 0.15, 1, 0.8414979747114353, 0.3833417840058648, 0.4091780035166267]\n",
      "[24, 16, 100, 3, 0.15, 1, 0.8413510843615499, 0.3663516375815304, 0.3973804000426645]\n"
     ]
    }
   ],
   "source": [
    "# Convert all elements in the list to float\n",
    "my_list_float = [[float(val) if '.' in val else int(val) for val in sublist] for sublist in parameter_result_list]\n",
    "\n",
    "# Sort the list based on the f1 values\n",
    "sorted_list = sorted(my_list_float, key=lambda x: x[6], reverse=True)\n",
    "\n",
    "# Print the sorted list\n",
    "for sublist in sorted_list:\n",
    "    print(sublist)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
