{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oeCE85RZaBN0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder,MinMaxScaler, StandardScaler\n",
        "import csv\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4dmgFLz8_ySQ"
      },
      "outputs": [],
      "source": [
        "# Set random seed for TensorFlow\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "# Set random seed for Python\n",
        "np.random.seed(123)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VHbe-92aulGr"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.set_random_seed(1)\n",
        "\n",
        "tf.config.experimental.enable_op_determinism()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfUWkEhXh31i",
        "outputId": "bd22d206-5a90-4b2a-a2ee-27113a06a81d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_1Ua7mbsbUOr"
      },
      "outputs": [],
      "source": [
        "\n",
        "data_all_county = pd.read_csv('/content/drive/My Drive/time_series/CA_data_lat_log_weekly.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_all_county['date'] = pd.to_datetime(data_all_county['date'])\n",
        "data_all_county['month'] = data_all_county['date'].dt.month\n",
        "data_all_county['month'] = data_all_county['month'].astype('category')"
      ],
      "metadata": {
        "id": "5jWpuLhEr0SS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkZqApXYXb-9"
      },
      "source": [
        "Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fIDCIpwSXXKt"
      },
      "outputs": [],
      "source": [
        "def ts_multi_data_prep(dataset, target, start, end, window, step_out):\n",
        "    X = []\n",
        "    y = []\n",
        "    start = start + window\n",
        "    if end is None:\n",
        "        end = len(dataset) - step_out\n",
        "        #end = len(dataset)\n",
        "    for i in range(start, end):\n",
        "        indices = range(i-window, i)\n",
        "        X.append(dataset[indices])\n",
        "\n",
        "        indicey = range(i, i+step_out) #revise the window definition\n",
        "        y.append(target[indicey])\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vp6wtluEXfZb"
      },
      "outputs": [],
      "source": [
        "def timeseries_evaluation_metrics_func(y_true, y_pred):\n",
        "    def mean_absolute_percentage_error(y_true, y_pred):\n",
        "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "    print('Evaluation metric results:-')\n",
        "    mse = metrics.mean_squared_error(y_true.flatten(), y_pred.flatten())\n",
        "    mae = metrics.mean_absolute_error(y_true.flatten(), y_pred.flatten())\n",
        "    rmse = np.sqrt(mse)\n",
        "    #mape = mean_absolute_percentage_error(y_true.flatten(), y_pred.flatten())\n",
        "    r2 = metrics.r2_score(y_true.flatten(), y_pred.flatten())\n",
        "    #print(f'MSE is : {mse}')\n",
        "    #print(f'MAE is : {mae}')\n",
        "    #print(f'RMSE is : {rmse}')\n",
        "    #print(f'MAPE is : {mape}')\n",
        "    #print(f'R2 is : {r2}\\n')\n",
        "    return mse, mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DZJVpSJ_49m2"
      },
      "outputs": [],
      "source": [
        "def timeseries_evaluation_metrics_binary(y_true, y_pred):\n",
        "    print('Evaluation metric results:-')\n",
        "    accuracy = accuracy_score(y_true.flatten(), y_pred.flatten())\n",
        "    precision = precision_score(y_true.flatten(), y_pred.flatten(), average='macro')\n",
        "    recall = recall_score(y_true.flatten(), y_pred.flatten(), average='macro')\n",
        "    f1 = f1_score(y_true.flatten(), y_pred.flatten(), average='macro')\n",
        "\n",
        "    #print(f'Accuracy: {accuracy}')\n",
        "    #print(f'Precision: {precision}')\n",
        "    #print(f'Recall: {recall}')\n",
        "    #print(f'F1-score: {f1}\\n')\n",
        "    return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xxvHTDdUqxyN"
      },
      "outputs": [],
      "source": [
        "def transform_county_data(x_data_array, y_data_array):\n",
        "    # Lists to store x_train_c and y_train_c arrays\n",
        "    x_c_list = []\n",
        "    y_c_list = []\n",
        "    # Divide the arrays into 'unique_fips_count' number of subarrays\n",
        "    x_subarrays = np.array_split(x_data_array, unique_fips_count, axis=0)\n",
        "    y_subarrays = np.array_split(y_data_array, unique_fips_count, axis=0)\n",
        "\n",
        "    # Combine x_subarrays and y_subarrays into tuples\n",
        "    data_tuples = [(x_subarray, y_subarray) for x_subarray, y_subarray in zip(x_subarrays, y_subarrays)]\n",
        "\n",
        "    # Print or use the data tuples as needed\n",
        "    for idx, data_tuple in enumerate(data_tuples):\n",
        "        x_window_c, y_window_c = ts_multi_data_prep(data_tuple[0],data_tuple[1], 0, None, hist_window, step_out)\n",
        "        # Append x_window_c and y_window_c arrays to lists\n",
        "        x_c_list.append(x_window_c)\n",
        "        y_c_list.append(y_window_c)\n",
        "\n",
        "    # Stack arrays in lists to create x_train_c and y_train_c\n",
        "    x_all_county = np.vstack(x_c_list)\n",
        "    y_all_county = np.vstack(y_c_list)\n",
        "\n",
        "    return x_all_county, y_all_county"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BJyFdGZFeeVi"
      },
      "outputs": [],
      "source": [
        "\n",
        "x_train_c, y_train_c, x_vali_c, y_vali_c, x_test_c, y_test_c = [], [], [], [], [], []\n",
        "unique_fips = data_all_county['fips'].unique()\n",
        "unique_fips_count = data_all_county['fips'].nunique()\n",
        "for fips in unique_fips:\n",
        "    # Extract dataframe for the current FIPS value\n",
        "    data_county = data_all_county[data_all_county['fips'] == fips]\n",
        "\n",
        "    X_data = data_county[['lat','lon','PRECTOT', 'PS', 'QV2M', 'T2M', 'T2MDEW', 'T2MWET',\n",
        "       'T2M_MAX', 'T2M_MIN', 'T2M_RANGE', 'TS', 'WS10M', 'WS10M_MAX',\n",
        "       'WS10M_MIN', 'WS10M_RANGE', 'WS50M', 'WS50M_MAX', 'WS50M_MIN',\n",
        "       'WS50M_RANGE', 'score','month']]\n",
        "    Y_data = data_county[['score']]\n",
        "    #train_val_test split 70%-10%-20%\n",
        "    n = len(X_data)\n",
        "\n",
        "    x_train_county = X_data[0:int(n*0.7)]\n",
        "    y_train_county = Y_data[0:int(n*0.7)]\n",
        "    x_vali_county = X_data[int(n*0.7):int(n*0.8)]\n",
        "    y_vali_county = Y_data[int(n*0.7):int(n*0.8)]\n",
        "    x_test_county = X_data[int(n*0.8):]\n",
        "    y_test_county = Y_data[int(n*0.8):]\n",
        "\n",
        "\n",
        "    if fips == 6001:\n",
        "        x_train_c, y_train_c, x_vali_c, y_vali_c, x_test_c, y_test_c = x_train_county, y_train_county, x_vali_county, y_vali_county, x_test_county, y_test_county\n",
        "\n",
        "    else:\n",
        "        x_train_c = np.concatenate((x_train_c, x_train_county), axis=0)\n",
        "        y_train_c = np.concatenate((y_train_c, y_train_county), axis=0)\n",
        "        x_vali_c = np.concatenate((x_vali_c, x_vali_county), axis=0)\n",
        "        y_vali_c = np.concatenate((y_vali_c, y_vali_county), axis=0)\n",
        "        x_test_c = np.concatenate((x_test_c, x_test_county), axis=0)\n",
        "        y_test_c = np.concatenate((y_test_c, y_test_county), axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MjYfAZXoYPE7"
      },
      "outputs": [],
      "source": [
        "X_scaler_train = MinMaxScaler()\n",
        "Y_scaler_train = MinMaxScaler()\n",
        "X_scaler_test = MinMaxScaler()\n",
        "Y_scaler_test = MinMaxScaler()\n",
        "X_scaler_vali = MinMaxScaler()\n",
        "Y_scaler_vali = MinMaxScaler()\n",
        "x_train_data = X_scaler_train.fit_transform(x_train_c)\n",
        "y_train_data = Y_scaler_train.fit_transform(y_train_c)\n",
        "x_vali_data = X_scaler_vali.fit_transform(x_vali_c)\n",
        "y_vali_data = Y_scaler_vali.fit_transform(y_vali_c)\n",
        "x_test_data = X_scaler_test.fit_transform(x_test_c)\n",
        "y_test_data = Y_scaler_test.fit_transform(y_test_c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NOygAYEWhEHl"
      },
      "outputs": [],
      "source": [
        "def modeling1(hist_window, step_out, filtersize, kernelsize, drop1, drop2, flag_report):\n",
        "\n",
        "  x_train, y_train = transform_county_data(x_train_data, y_train_data)\n",
        "  x_vali, y_vali = transform_county_data(x_vali_data, y_vali_data)\n",
        "  x_test, y_test = transform_county_data(x_test_data, y_test_data)\n",
        "  batch_size = 128\n",
        "  buffer_size = 256\n",
        "\n",
        "  train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "  train_data = train_data.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
        "\n",
        "  val_data = tf.data.Dataset.from_tensor_slices((x_vali, y_vali))\n",
        "  val_data = val_data.batch(batch_size).repeat()\n",
        "\n",
        "  cnn_model = tf.keras.models.Sequential()\n",
        "  cnn_model.add(tf.keras.layers.Conv1D(filters=filtersize, kernel_size=kernelsize, activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])))\n",
        "  cnn_model.add(tf.keras.layers.MaxPool1D(pool_size=2))\n",
        "  cnn_model.add(tf.keras.layers.Dropout(drop1))\n",
        "  cnn_model.add(tf.keras.layers.Flatten())\n",
        "  cnn_model.add(tf.keras.layers.Dense(30, activation='relu'))\n",
        "  cnn_model.add(tf.keras.layers.Dropout(drop2))\n",
        "  cnn_model.add(tf.keras.layers.Dense(units=step_out))\n",
        "  cnn_model.compile(optimizer='adam', loss='mae')\n",
        "\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='min')\n",
        "  history = cnn_model.fit(train_data, epochs=150, steps_per_epoch=50, validation_data=val_data, validation_steps=150, verbose=0, callbacks=[early_stopping])\n",
        "  y_test_pred = cnn_model.predict(x_test)\n",
        "  y_test_pred_Inverse = Y_scaler_test.inverse_transform(y_test_pred)\n",
        "  y_test_pred_Inverse_ordinal = np.round(y_test_pred_Inverse).astype(int)\n",
        "  y_test_reshaped = np.squeeze(y_test, axis=-1)\n",
        "  y_test_Inverse = Y_scaler_test.inverse_transform(y_test_reshaped)\n",
        "  y_test_Inverse_ordinal = np.round(y_test_Inverse).astype(int)\n",
        "  mse, mae = timeseries_evaluation_metrics_func(y_test_Inverse,y_test_pred_Inverse)\n",
        "  threshold = 2.5\n",
        "  y_test_Inverse_binary = np.where(y_test_Inverse >= threshold, 1, 0)\n",
        "  y_test_pred_Inverse_binary = np.where(y_test_pred_Inverse >= threshold, 1, 0)\n",
        "  f1 = timeseries_evaluation_metrics_binary(y_test_Inverse_binary,y_test_pred_Inverse_binary)\n",
        "  if flag_report:\n",
        "    classification_metrics = classification_report(y_test_Inverse_binary.flatten(),y_test_pred_Inverse_binary.flatten())\n",
        "    print(classification_metrics)\n",
        "\n",
        "  print(f'filtersize: {filtersize}, kernelsize: {kernelsize}, Dropout1: {drop1}, Dropout2: {drop2}, F1: {f1}, MSE: {mse}, MAE: {mae}')\n",
        "  return f1, mse, mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-cWA8JVdbQw",
        "outputId": "a8ac97a1-75ef-4928-804d-60160860b3f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32: early stopping\n",
            "323/323 [==============================] - 1s 2ms/step\n",
            "Evaluation metric results:-\n",
            "Evaluation metric results:-\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97    110778\n",
            "           1       0.94      0.56      0.70     13110\n",
            "\n",
            "    accuracy                           0.95    123888\n",
            "   macro avg       0.94      0.78      0.83    123888\n",
            "weighted avg       0.95      0.95      0.94    123888\n",
            "\n",
            "filtersize: 10, kernelsize: 3, Dropout1: 0.1, Dropout2: 0.1, F1: 0.834965685600569, MSE: 0.40222482923754554, MAE: 0.3790649861196106\n",
            "Epoch 22: early stopping\n",
            "323/323 [==============================] - 1s 2ms/step\n",
            "Evaluation metric results:-\n",
            "Evaluation metric results:-\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.97    110778\n",
            "           1       0.90      0.61      0.73     13110\n",
            "\n",
            "    accuracy                           0.95    123888\n",
            "   macro avg       0.93      0.80      0.85    123888\n",
            "weighted avg       0.95      0.95      0.95    123888\n",
            "\n",
            "filtersize: 10, kernelsize: 4, Dropout1: 0.1, Dropout2: 0.1, F1: 0.84911978094232, MSE: 0.4290468515046707, MAE: 0.4281373958403994\n",
            "Epoch 26: early stopping\n",
            "323/323 [==============================] - 1s 2ms/step\n",
            "Evaluation metric results:-\n",
            "Evaluation metric results:-\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.98    110778\n",
            "           1       0.84      0.72      0.77     13110\n",
            "\n",
            "    accuracy                           0.96    123888\n",
            "   macro avg       0.91      0.85      0.87    123888\n",
            "weighted avg       0.95      0.96      0.95    123888\n",
            "\n",
            "filtersize: 10, kernelsize: 5, Dropout1: 0.1, Dropout2: 0.1, F1: 0.8749162573997669, MSE: 0.36537235143499813, MAE: 0.38188375515207823\n",
            "Epoch 23: early stopping\n",
            "323/323 [==============================] - 1s 2ms/step\n",
            "Evaluation metric results:-\n",
            "Evaluation metric results:-\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.99      0.97    110778\n",
            "           1       0.88      0.48      0.62     13110\n",
            "\n",
            "    accuracy                           0.94    123888\n",
            "   macro avg       0.91      0.74      0.79    123888\n",
            "weighted avg       0.94      0.94      0.93    123888\n",
            "\n",
            "filtersize: 32, kernelsize: 3, Dropout1: 0.1, Dropout2: 0.1, F1: 0.7940485800960773, MSE: 0.4493505441320324, MAE: 0.4168063310122966\n",
            "Epoch 19: early stopping\n",
            "323/323 [==============================] - 1s 2ms/step\n",
            "Evaluation metric results:-\n",
            "Evaluation metric results:-\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.97    110778\n",
            "           1       0.85      0.61      0.71     13110\n",
            "\n",
            "    accuracy                           0.95    123888\n",
            "   macro avg       0.90      0.80      0.84    123888\n",
            "weighted avg       0.94      0.95      0.94    123888\n",
            "\n",
            "filtersize: 32, kernelsize: 4, Dropout1: 0.1, Dropout2: 0.1, F1: 0.8426570759112698, MSE: 0.4241608177090393, MAE: 0.3972481453966835\n",
            "Epoch 20: early stopping\n",
            "323/323 [==============================] - 1s 2ms/step\n",
            "Evaluation metric results:-\n",
            "Evaluation metric results:-\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.97    110778\n",
            "           1       0.83      0.71      0.76     13110\n",
            "\n",
            "    accuracy                           0.95    123888\n",
            "   macro avg       0.90      0.85      0.87    123888\n",
            "weighted avg       0.95      0.95      0.95    123888\n",
            "\n",
            "filtersize: 32, kernelsize: 5, Dropout1: 0.1, Dropout2: 0.1, F1: 0.8694547014994938, MSE: 0.40949392276568347, MAE: 0.38840285834819727\n",
            "Epoch 19: early stopping\n",
            "323/323 [==============================] - 1s 2ms/step\n",
            "Evaluation metric results:-\n",
            "Evaluation metric results:-\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.97    110778\n",
            "           1       0.80      0.70      0.75     13110\n",
            "\n",
            "    accuracy                           0.95    123888\n",
            "   macro avg       0.88      0.84      0.86    123888\n",
            "weighted avg       0.95      0.95      0.95    123888\n",
            "\n",
            "filtersize: 64, kernelsize: 3, Dropout1: 0.1, Dropout2: 0.1, F1: 0.8603357757596977, MSE: 0.3668929803686541, MAE: 0.36890938483406566\n",
            "Epoch 19: early stopping\n",
            "323/323 [==============================] - 1s 2ms/step\n",
            "Evaluation metric results:-\n",
            "Evaluation metric results:-\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      1.00      0.96    110778\n",
            "           1       0.92      0.33      0.49     13110\n",
            "\n",
            "    accuracy                           0.93    123888\n",
            "   macro avg       0.92      0.66      0.72    123888\n",
            "weighted avg       0.93      0.93      0.91    123888\n",
            "\n",
            "filtersize: 64, kernelsize: 4, Dropout1: 0.1, Dropout2: 0.1, F1: 0.7236089173551418, MSE: 0.46806839322742405, MAE: 0.4305305244081411\n",
            "Epoch 19: early stopping\n",
            "323/323 [==============================] - 1s 2ms/step\n",
            "Evaluation metric results:-\n",
            "Evaluation metric results:-\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.99      0.97    110778\n",
            "           1       0.90      0.52      0.66     13110\n",
            "\n",
            "    accuracy                           0.94    123888\n",
            "   macro avg       0.92      0.76      0.81    123888\n",
            "weighted avg       0.94      0.94      0.94    123888\n",
            "\n",
            "filtersize: 64, kernelsize: 5, Dropout1: 0.1, Dropout2: 0.1, F1: 0.8146858692406492, MSE: 0.39148586537831415, MAE: 0.3849315434792824\n",
            "# Hist_Window   Step_Out   filtersize, kernelsize,   Dropout1   Dropout2   F1    MSE       MAE\n",
            "30            12         10      3       0.1        0.1        0.8350     0.4022     0.3791    \n",
            "30            12         10      4       0.1        0.1        0.8491     0.4290     0.4281    \n",
            "30            12         10      5       0.1        0.1        0.8749     0.3654     0.3819    \n",
            "30            12         32      3       0.1        0.1        0.7940     0.4494     0.4168    \n",
            "30            12         32      4       0.1        0.1        0.8427     0.4242     0.3972    \n",
            "30            12         32      5       0.1        0.1        0.8695     0.4095     0.3884    \n",
            "30            12         64      3       0.1        0.1        0.8603     0.3669     0.3689    \n",
            "30            12         64      4       0.1        0.1        0.7236     0.4681     0.4305    \n",
            "30            12         64      5       0.1        0.1        0.8147     0.3915     0.3849    \n"
          ]
        }
      ],
      "source": [
        "parameter_result_list1 = []\n",
        "hist_window = 30\n",
        "step_out = 12\n",
        "\n",
        "for filtersize in [10, 32, 64]:\n",
        "    for kernelsize in [3, 4, 5]:\n",
        "        for drop1 in [0.1]:\n",
        "            for drop2 in [0.1]:\n",
        "                f1, mse, mae = modeling1(hist_window, step_out, filtersize, kernelsize, drop1, drop2, 1)\n",
        "                parameter_result_list1.append((hist_window, step_out, filtersize, kernelsize, drop1, drop2, f1, mse, mae))\n",
        "\n",
        "# Printing the list with comment lines indicating parameter titles\n",
        "print(\"# Hist_Window   Step_Out   filtersize, kernelsize,   Dropout1   Dropout2   F1    MSE       MAE\")\n",
        "for params in parameter_result_list1:\n",
        "    print(\"{:<13} {:<10} {:<7} {:<7} {:<10} {:<10} {:<10.4f} {:<10.4f} {:<10.4f}\".format(*params))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ivpbVeCFIqdJ"
      },
      "outputs": [],
      "source": [
        "# Saving the parameter_result_list to a CSV file\n",
        "with open('/content/drive/My Drive/time_series/parameter_result_list1_cnn.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(parameter_result_list1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JogzhJEsAzTP"
      },
      "outputs": [],
      "source": [
        "# Loading the parameter_result_list from the CSV file\n",
        "parameter_result_list = []\n",
        "\n",
        "with open('/content/drive/My Drive/time_series/parameter_result_list1_cnn.csv', 'r', newline='') as file:\n",
        "    reader = csv.reader(file)\n",
        "    for row in reader:\n",
        "        parameter_result_list.append(row)\n",
        "#parameter_result_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LLxJO_CvTu8q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18a352e9-eeed-47e7-81b1-8ba3dfaad73b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30, 12, 10, 5, 0.1, 0.1, 0.8749162573997669, 0.36537235143499813, 0.38188375515207823]\n",
            "[30, 12, 32, 5, 0.1, 0.1, 0.8694547014994938, 0.40949392276568347, 0.38840285834819727]\n",
            "[30, 12, 64, 3, 0.1, 0.1, 0.8603357757596977, 0.3668929803686541, 0.36890938483406566]\n",
            "[30, 12, 10, 4, 0.1, 0.1, 0.84911978094232, 0.4290468515046707, 0.4281373958403994]\n",
            "[30, 12, 32, 4, 0.1, 0.1, 0.8426570759112698, 0.4241608177090393, 0.3972481453966835]\n",
            "[30, 12, 10, 3, 0.1, 0.1, 0.834965685600569, 0.40222482923754554, 0.3790649861196106]\n",
            "[30, 12, 64, 5, 0.1, 0.1, 0.8146858692406492, 0.39148586537831415, 0.3849315434792824]\n",
            "[30, 12, 32, 3, 0.1, 0.1, 0.7940485800960773, 0.4493505441320324, 0.4168063310122966]\n",
            "[30, 12, 64, 4, 0.1, 0.1, 0.7236089173551418, 0.46806839322742405, 0.4305305244081411]\n"
          ]
        }
      ],
      "source": [
        "# Convert all elements in the list to float\n",
        "my_list_float = [[float(val) if '.' in val else int(val) for val in sublist] for sublist in parameter_result_list]\n",
        "\n",
        "# Sort the list based on the f1 values\n",
        "sorted_list = sorted(my_list_float, key=lambda x: x[6], reverse=True)\n",
        "\n",
        "# Print the sorted list\n",
        "for sublist in sorted_list:\n",
        "    print(sublist)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}